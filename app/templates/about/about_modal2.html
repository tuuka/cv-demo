<p>
  Как создать простейшее Flask-приложение для распознавания объектов с использованием модели PyTorch вполне подробно
  расписано <a href="https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html">здесь</a>. Остается только
  адаптировать этот туториал под использование <span>blueprint</span>-ов, определиться с использьзуемыми моделями и
  разделить все это на три раздела.
</p>
<p>
  Ориентируясь на
  <a href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">этот</a> и
  <a href="https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html">этот</a> туториалы, не составит особого
  труда подстроить модели под любой датасет. Для начала же, я решил ограничиться предобученными моделями PyTorch
  torchvision. Для задачи распознавания изображений этими моделями будут MobileNetV2, ResNet34 и ResNet101; для
  семантический сегментации - ResNet101FCN и ResNet101Deeplab3 и для детекции объектов и сегментации инстансов пока одна
  лишь MaskRCNN.
</p>
<p>
  Flask-приложение состоит из серверной и клиентской части. На текущем этапе приложение будет работать локально на
  встроенном во Flask веб-сервере. Подробности, касающиеся структуры и функционирования Flask-приложения можно
  подчеркнуть из ранее упоминавшейся мной
  <a href="https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-i-hello-world">книги</a> Мигуэля Гринберга,
  для которой есть отличный перевод на русский на <a href="https://habr.com/ru/post/346306/">хабре</a>.
</p>
<p>
  Клиентская часть предоставляет пользователю возможность выбора файла изображения с его отображением в области
  предпросмотра и одну из предложенных моделей. Выбранные данные посредством JS-запроса отправляются в серверную часть в
  predict-route соответствующего раздела. Полученные результаты предсказаний отображаются динамически на той же странице
  поверх области предпросмотра.
</p>
<p>
  В серверной части, для каждого из разделов имеется свой файл маршрутов <span>route.py</span>, который отрисовывает
  веб-страницу соответствующего раздела, а также осуществляет API-интерфейс, принимающий от клиента данные для обработки
  и отправляющий обратно результат. Также для каждого из разделов на этом этапе я создал файл моделей
  <span>models.py</span> и файл <span>utils.py</span>, в котором, собственно, и осуществляется обработка изображения
  соответствующей моделью.
</p>
<p>
  Предполагалось, что веса моделей могут загружаться как по url-ссылке так и из google-drive. В файле
  <span>/app/utils.py</span> реализована такая возможность.
</p>
<p>
  В процессе работы над приложением код несколько раз рефакторился, но общая структура проекта осталась без изменений.
  Код самого первого рабочего варианта можно найти в истории коммитов репозитория, например
  <a href="https://github.com/tuuka/cv-demo/tree/99ade1ecd23bf6bd6ac2afc5a082675cf5e2ecc5">здесь</a>. Тоже самое можно
  получить посредством <span>git checkout v04</span> клонированного репозитория. Приложение можно запустить локально из
  командной строки в созданной виртульной среде со всеми необходимыми зависимостями командой
  <span>python cv-demo.py</span>.
</p>
<p>
  Описанное выше потребовало изучения довольно большого количества новых для меня материалов. Сами шаблоны страниц со
  стилями на начальном этапе топорно были слеплены по принципу "лишь бы работало". Javascript отнял гораздо больше
  времени. В итоге, пытаясь добиться универсальности и простоты, я полностью отказался от ajax-запросов в пользу
  универсального и глобального
  <a href="https://developer.mozilla.org/ru/docs/Web/API/Fetch_API/Using_Fetch">fetch</a>, который позволил как
  принимать файлы изображения со всех желаемых источников (url-ссылки, локальное хранилище, камера, буфер обмена), так и
  непосредственно общаться с серверной частью. Реализация основного функционала отняла довольно много времени, но, по
  большому счету, все необходимое широко освящено в различных источниках с хорошими примерами, поэтому расписывать
  подробности здесь я не буду, все можно посмотреть в вышеуказанном github-репозитории.
</p>
