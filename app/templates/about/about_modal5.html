<p>
  Из опытов, описанных в предыдущих разделах, становится совершенно очевидно, что для размещения сложных моделей PyTorch
  "как есть" на бесплатной основе практически невозможно. Лишь один файл сохраненных весов рассматриваемой MaskRCNN
  занимает более 170Мб, а загруженная в память модель, после обработки изображения, может занимать до 2-3Гб. Таким
  образом требуется виртуальная машина с, как минимум, 3Гб ОЗУ, либо нужно рассматривать варианты уменьшения самой
  модели.
</p>
<p>
  Один из наиболее распространенных и сравнительно простых способой уменьшения размеров и требований к ОЗУ для
  предобученных моделей является
  <a href="https://pytorch.org/docs/stable/quantization.html">квантизация</a>, общий смысл которой сводится к уменьшению
  размерности весов и данных, пропускаемых через слои нейронной сети. В общем случае веса модели (соответственно и
  данные) имеют точность float32, различные методы квантизации предполагают уменьшение точности как до float16, так и до
  INT8. Второй вариант реализован в PyTorch, поэтому его и будем применять для уменьшения размера наиболее сложной
  (пока) в проекте модели MaskRCNN на базе ResNet50.
</p>
<p>
  На сегодняшний момент в PyTorch имеются несколько органичений. Во-первых, полностью отсутствует поддержка GPU для
  квантизированных моделей, что не критично для этого проекта. Во-вторых, для квантизации поддерживаются два бэкэнда:
  <a href="https://github.com/pytorch/FBGEMM">FBGEMM</a> для процессоров X86 с поддержкой инструкций AVX2 (чего на моем
  домашнем динозавре нет, из-за чего все эксперименты пришлось проводить на Colab-е) и
  <a href="https://github.com/pytorch/QNNPACK">QNNPACK</a> для процессоров ARM (обычно в мобильных устройствах). Модель
  можно квантизировать либо тем, либо другим бэкэндом. Как оказалось, Google Colab поддерживает оба бэкэнда, а Heroku
  только QNNPACK. Посмотреть список поддерживаемых системой бэкэндов можно из питона командой
  <span>torch.backends.quantized.supported_engines</span>.
</p>
<p>
  Итак, не вдаваясь в глубокие
  <a href="https://towardsdatascience.com/speeding-up-deep-learning-with-quantization-3fe3538cbb9">подробности</a>,
  PyTorch предлагает
  <a href="https://pytorch.org/docs/stable/quantization.html#quantization-workflows">три способа</a>
  квантизации и предоставляет несколько
  <a href="https://pytorch.org/tutorials/#model-optimization">туториалов</a> по их применению. Ввиду того, что
  динамическая квантизация нацелена, в первую очередь, на RNN и Linear слои, а "дотренировка" квантизированной модели на
  этом этапе не планируется, я остановился на статической квантизации, которая предполагает следующую последовательность
  действий:
</p>
<ol class="modal-list">
  <li>
    Подготовка модели. Включает в себя, прежде всего, добавление в модель QuantStub и DeQuantStub модулей, выполняющих,
    неспосредственно, преобразование входных данных из float32 в INT8 и обратно; изменение forward-метода, чтобы
    осуществлялся вызов этих модулей, соответственно в forward-методе QuantStub вызывается перед самым первым слоем
    сети, а DeQuantStub после самого последнего, а также добавление функции слияния модулей (о смысле которой чут ниже).
    То есть, входные данные в самом начале преобразуются в INT8, после чего пропускаются через все модули модели, после
    чего преобразуются обратно во float32.
  </li>
  <li>
    Слияние модулей. Статическая квантизация преобразует float32 тензоры в INT8 в соответствии с выражением
    <br />
    <span
      ><svg
        xmlns="http://www.w3.org/2000/svg"
        xmlns:xlink="http://www.w3.org/1999/xlink"
        width="227pt"
        height="19pt"
        viewBox="0 0 227 19"
        version="1.1"
      >
        <defs>
          <g>
            <symbol overflow="visible" id="glyph0-0">
              <path
                style="stroke: none;"
                d="M 4.359375 -0.0625 C 5.90625 -0.640625 7.375 -2.421875 7.375 -4.34375 C 7.375 -5.953125 6.3125 -7.03125 4.828125 -7.03125 C 2.6875 -7.03125 0.484375 -4.765625 0.484375 -2.4375 C 0.484375 -0.78125 1.609375 0.21875 3.046875 0.21875 C 3.296875 0.21875 3.625 0.171875 4.015625 0.0625 C 3.984375 0.6875 3.984375 0.703125 3.984375 0.84375 C 3.984375 1.15625 3.984375 1.9375 4.8125 1.9375 C 5.984375 1.9375 6.46875 0.109375 6.46875 0 C 6.46875 -0.0625 6.40625 -0.09375 6.359375 -0.09375 C 6.28125 -0.09375 6.265625 -0.046875 6.234375 0.015625 C 6 0.71875 5.421875 0.96875 5.078125 0.96875 C 4.609375 0.96875 4.46875 0.703125 4.359375 -0.0625 Z M 2.484375 -0.140625 C 1.703125 -0.453125 1.359375 -1.21875 1.359375 -2.125 C 1.359375 -2.8125 1.625 -4.234375 2.375 -5.296875 C 3.109375 -6.3125 4.046875 -6.78125 4.78125 -6.78125 C 5.765625 -6.78125 6.5 -6 6.5 -4.671875 C 6.5 -3.671875 5.984375 -1.328125 4.3125 -0.40625 C 4.265625 -0.75 4.171875 -1.46875 3.4375 -1.46875 C 2.90625 -1.46875 2.421875 -0.984375 2.421875 -0.453125 C 2.421875 -0.265625 2.484375 -0.15625 2.484375 -0.140625 Z M 3.09375 -0.03125 C 2.953125 -0.03125 2.640625 -0.03125 2.640625 -0.453125 C 2.640625 -0.859375 3.015625 -1.25 3.4375 -1.25 C 3.859375 -1.25 4.046875 -1.015625 4.046875 -0.40625 C 4.046875 -0.265625 4.03125 -0.25 3.9375 -0.203125 C 3.671875 -0.09375 3.375 -0.03125 3.09375 -0.03125 Z M 3.09375 -0.03125 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-1">
              <path
                style="stroke: none;"
                d="M 3.328125 -3.015625 C 3.390625 -3.265625 3.625 -4.1875 4.3125 -4.1875 C 4.359375 -4.1875 4.609375 -4.1875 4.8125 -4.0625 C 4.53125 -4 4.34375 -3.765625 4.34375 -3.515625 C 4.34375 -3.359375 4.453125 -3.171875 4.71875 -3.171875 C 4.9375 -3.171875 5.25 -3.34375 5.25 -3.75 C 5.25 -4.265625 4.671875 -4.40625 4.328125 -4.40625 C 3.75 -4.40625 3.40625 -3.875 3.28125 -3.65625 C 3.03125 -4.3125 2.5 -4.40625 2.203125 -4.40625 C 1.171875 -4.40625 0.59375 -3.125 0.59375 -2.875 C 0.59375 -2.765625 0.703125 -2.765625 0.71875 -2.765625 C 0.796875 -2.765625 0.828125 -2.796875 0.84375 -2.875 C 1.1875 -3.9375 1.84375 -4.1875 2.1875 -4.1875 C 2.375 -4.1875 2.71875 -4.09375 2.71875 -3.515625 C 2.71875 -3.203125 2.546875 -2.546875 2.1875 -1.140625 C 2.03125 -0.53125 1.671875 -0.109375 1.234375 -0.109375 C 1.171875 -0.109375 0.953125 -0.109375 0.734375 -0.234375 C 0.984375 -0.296875 1.203125 -0.5 1.203125 -0.78125 C 1.203125 -1.046875 0.984375 -1.125 0.84375 -1.125 C 0.53125 -1.125 0.296875 -0.875 0.296875 -0.546875 C 0.296875 -0.09375 0.78125 0.109375 1.21875 0.109375 C 1.890625 0.109375 2.25 -0.59375 2.265625 -0.640625 C 2.390625 -0.28125 2.75 0.109375 3.34375 0.109375 C 4.375 0.109375 4.9375 -1.171875 4.9375 -1.421875 C 4.9375 -1.53125 4.859375 -1.53125 4.828125 -1.53125 C 4.734375 -1.53125 4.71875 -1.484375 4.6875 -1.421875 C 4.359375 -0.34375 3.6875 -0.109375 3.375 -0.109375 C 2.984375 -0.109375 2.828125 -0.421875 2.828125 -0.765625 C 2.828125 -0.984375 2.875 -1.203125 2.984375 -1.640625 Z M 3.328125 -3.015625 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-2">
              <path
                style="stroke: none;"
                d="M 2.03125 -0.015625 C 2.03125 -0.671875 1.78125 -1.0625 1.390625 -1.0625 C 1.0625 -1.0625 0.859375 -0.8125 0.859375 -0.53125 C 0.859375 -0.265625 1.0625 0 1.390625 0 C 1.5 0 1.640625 -0.046875 1.734375 -0.125 C 1.765625 -0.15625 1.78125 -0.15625 1.78125 -0.15625 C 1.796875 -0.15625 1.796875 -0.15625 1.796875 -0.015625 C 1.796875 0.734375 1.453125 1.328125 1.125 1.65625 C 1.015625 1.765625 1.015625 1.78125 1.015625 1.8125 C 1.015625 1.890625 1.0625 1.921875 1.109375 1.921875 C 1.21875 1.921875 2.03125 1.15625 2.03125 -0.015625 Z M 2.03125 -0.015625 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-3">
              <path
                style="stroke: none;"
                d="M 3.890625 -3.734375 C 3.625 -3.71875 3.421875 -3.5 3.421875 -3.28125 C 3.421875 -3.140625 3.515625 -2.984375 3.734375 -2.984375 C 3.953125 -2.984375 4.1875 -3.15625 4.1875 -3.546875 C 4.1875 -4 3.765625 -4.40625 3 -4.40625 C 1.6875 -4.40625 1.3125 -3.390625 1.3125 -2.953125 C 1.3125 -2.171875 2.046875 -2.03125 2.34375 -1.96875 C 2.859375 -1.859375 3.375 -1.75 3.375 -1.203125 C 3.375 -0.953125 3.15625 -0.109375 1.953125 -0.109375 C 1.8125 -0.109375 1.046875 -0.109375 0.8125 -0.640625 C 1.203125 -0.59375 1.453125 -0.890625 1.453125 -1.171875 C 1.453125 -1.390625 1.28125 -1.515625 1.078125 -1.515625 C 0.8125 -1.515625 0.515625 -1.3125 0.515625 -0.859375 C 0.515625 -0.296875 1.09375 0.109375 1.9375 0.109375 C 3.5625 0.109375 3.953125 -1.09375 3.953125 -1.546875 C 3.953125 -1.90625 3.765625 -2.15625 3.640625 -2.265625 C 3.375 -2.546875 3.078125 -2.609375 2.640625 -2.6875 C 2.28125 -2.765625 1.890625 -2.84375 1.890625 -3.296875 C 1.890625 -3.578125 2.125 -4.1875 3 -4.1875 C 3.25 -4.1875 3.75 -4.109375 3.890625 -3.734375 Z M 3.890625 -3.734375 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-4">
              <path
                style="stroke: none;"
                d="M 3.953125 -3.78125 C 3.78125 -3.78125 3.65625 -3.78125 3.515625 -3.65625 C 3.34375 -3.5 3.328125 -3.328125 3.328125 -3.265625 C 3.328125 -3.015625 3.515625 -2.90625 3.703125 -2.90625 C 3.984375 -2.90625 4.25 -3.15625 4.25 -3.546875 C 4.25 -4.03125 3.78125 -4.40625 3.078125 -4.40625 C 1.734375 -4.40625 0.40625 -2.984375 0.40625 -1.578125 C 0.40625 -0.671875 0.984375 0.109375 2.03125 0.109375 C 3.453125 0.109375 4.28125 -0.953125 4.28125 -1.0625 C 4.28125 -1.125 4.234375 -1.203125 4.171875 -1.203125 C 4.109375 -1.203125 4.09375 -1.171875 4.03125 -1.09375 C 3.25 -0.109375 2.15625 -0.109375 2.046875 -0.109375 C 1.421875 -0.109375 1.140625 -0.59375 1.140625 -1.203125 C 1.140625 -1.609375 1.34375 -2.578125 1.6875 -3.1875 C 2 -3.765625 2.546875 -4.1875 3.09375 -4.1875 C 3.421875 -4.1875 3.8125 -4.0625 3.953125 -3.78125 Z M 3.953125 -3.78125 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-5">
              <path
                style="stroke: none;"
                d="M 3.71875 -3.765625 C 3.53125 -4.140625 3.25 -4.40625 2.796875 -4.40625 C 1.640625 -4.40625 0.40625 -2.9375 0.40625 -1.484375 C 0.40625 -0.546875 0.953125 0.109375 1.71875 0.109375 C 1.921875 0.109375 2.421875 0.0625 3.015625 -0.640625 C 3.09375 -0.21875 3.453125 0.109375 3.921875 0.109375 C 4.28125 0.109375 4.5 -0.125 4.671875 -0.4375 C 4.828125 -0.796875 4.96875 -1.40625 4.96875 -1.421875 C 4.96875 -1.53125 4.875 -1.53125 4.84375 -1.53125 C 4.75 -1.53125 4.734375 -1.484375 4.703125 -1.34375 C 4.53125 -0.703125 4.359375 -0.109375 3.953125 -0.109375 C 3.671875 -0.109375 3.65625 -0.375 3.65625 -0.5625 C 3.65625 -0.78125 3.671875 -0.875 3.78125 -1.3125 C 3.890625 -1.71875 3.90625 -1.828125 4 -2.203125 L 4.359375 -3.59375 C 4.421875 -3.875 4.421875 -3.890625 4.421875 -3.9375 C 4.421875 -4.109375 4.3125 -4.203125 4.140625 -4.203125 C 3.890625 -4.203125 3.75 -3.984375 3.71875 -3.765625 Z M 3.078125 -1.1875 C 3.015625 -1 3.015625 -0.984375 2.875 -0.8125 C 2.4375 -0.265625 2.03125 -0.109375 1.75 -0.109375 C 1.25 -0.109375 1.109375 -0.65625 1.109375 -1.046875 C 1.109375 -1.546875 1.421875 -2.765625 1.65625 -3.234375 C 1.96875 -3.8125 2.40625 -4.1875 2.8125 -4.1875 C 3.453125 -4.1875 3.59375 -3.375 3.59375 -3.3125 C 3.59375 -3.25 3.578125 -3.1875 3.5625 -3.140625 Z M 3.078125 -1.1875 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-6">
              <path
                style="stroke: none;"
                d="M 2.578125 -6.8125 C 2.578125 -6.8125 2.578125 -6.921875 2.4375 -6.921875 C 2.21875 -6.921875 1.484375 -6.84375 1.21875 -6.8125 C 1.140625 -6.8125 1.03125 -6.796875 1.03125 -6.609375 C 1.03125 -6.5 1.140625 -6.5 1.28125 -6.5 C 1.765625 -6.5 1.78125 -6.40625 1.78125 -6.328125 L 1.75 -6.125 L 0.484375 -1.140625 C 0.453125 -1.03125 0.4375 -0.96875 0.4375 -0.8125 C 0.4375 -0.234375 0.875 0.109375 1.34375 0.109375 C 1.671875 0.109375 1.921875 -0.09375 2.09375 -0.453125 C 2.265625 -0.828125 2.390625 -1.40625 2.390625 -1.421875 C 2.390625 -1.53125 2.296875 -1.53125 2.265625 -1.53125 C 2.171875 -1.53125 2.15625 -1.484375 2.140625 -1.34375 C 1.96875 -0.703125 1.78125 -0.109375 1.375 -0.109375 C 1.078125 -0.109375 1.078125 -0.421875 1.078125 -0.5625 C 1.078125 -0.8125 1.09375 -0.859375 1.140625 -1.046875 Z M 2.578125 -6.8125 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-7">
              <path
                style="stroke: none;"
                d="M 1.859375 -2.296875 C 2.15625 -2.296875 2.890625 -2.328125 3.390625 -2.53125 C 4.09375 -2.828125 4.140625 -3.421875 4.140625 -3.5625 C 4.140625 -4 3.765625 -4.40625 3.078125 -4.40625 C 1.96875 -4.40625 0.453125 -3.4375 0.453125 -1.6875 C 0.453125 -0.671875 1.046875 0.109375 2.03125 0.109375 C 3.453125 0.109375 4.28125 -0.953125 4.28125 -1.0625 C 4.28125 -1.125 4.234375 -1.203125 4.171875 -1.203125 C 4.109375 -1.203125 4.09375 -1.171875 4.03125 -1.09375 C 3.25 -0.109375 2.15625 -0.109375 2.046875 -0.109375 C 1.265625 -0.109375 1.171875 -0.953125 1.171875 -1.265625 C 1.171875 -1.390625 1.1875 -1.6875 1.328125 -2.296875 Z M 1.390625 -2.515625 C 1.78125 -4.03125 2.8125 -4.1875 3.078125 -4.1875 C 3.53125 -4.1875 3.8125 -3.890625 3.8125 -3.5625 C 3.8125 -2.515625 2.21875 -2.515625 1.796875 -2.515625 Z M 1.390625 -2.515625 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-8">
              <path
                style="stroke: none;"
                d="M 1.328125 -0.828125 C 1.859375 -1.40625 2.15625 -1.65625 2.515625 -1.96875 C 2.515625 -1.96875 3.125 -2.5 3.484375 -2.859375 C 4.4375 -3.78125 4.65625 -4.265625 4.65625 -4.3125 C 4.65625 -4.40625 4.5625 -4.40625 4.546875 -4.40625 C 4.46875 -4.40625 4.453125 -4.390625 4.390625 -4.296875 C 4.09375 -3.8125 3.890625 -3.65625 3.65625 -3.65625 C 3.40625 -3.65625 3.296875 -3.8125 3.140625 -3.984375 C 2.953125 -4.203125 2.78125 -4.40625 2.453125 -4.40625 C 1.703125 -4.40625 1.25 -3.484375 1.25 -3.265625 C 1.25 -3.21875 1.28125 -3.15625 1.359375 -3.15625 C 1.453125 -3.15625 1.46875 -3.203125 1.5 -3.265625 C 1.6875 -3.734375 2.265625 -3.734375 2.34375 -3.734375 C 2.546875 -3.734375 2.734375 -3.671875 2.96875 -3.59375 C 3.375 -3.4375 3.484375 -3.4375 3.734375 -3.4375 C 3.375 -3.015625 2.546875 -2.296875 2.359375 -2.140625 L 1.453125 -1.296875 C 0.78125 -0.625 0.421875 -0.0625 0.421875 0.015625 C 0.421875 0.109375 0.53125 0.109375 0.546875 0.109375 C 0.625 0.109375 0.640625 0.09375 0.703125 -0.015625 C 0.9375 -0.375 1.234375 -0.640625 1.5625 -0.640625 C 1.78125 -0.640625 1.890625 -0.546875 2.140625 -0.265625 C 2.296875 -0.046875 2.484375 0.109375 2.765625 0.109375 C 3.765625 0.109375 4.34375 -1.15625 4.34375 -1.421875 C 4.34375 -1.46875 4.296875 -1.53125 4.21875 -1.53125 C 4.125 -1.53125 4.109375 -1.46875 4.078125 -1.390625 C 3.84375 -0.75 3.203125 -0.5625 2.875 -0.5625 C 2.6875 -0.5625 2.5 -0.625 2.296875 -0.6875 C 1.953125 -0.8125 1.796875 -0.859375 1.59375 -0.859375 C 1.578125 -0.859375 1.421875 -0.859375 1.328125 -0.828125 Z M 1.328125 -0.828125 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-9">
              <path
                style="stroke: none;"
                d="M 0.875 -0.59375 C 0.84375 -0.4375 0.78125 -0.203125 0.78125 -0.15625 C 0.78125 0.015625 0.921875 0.109375 1.078125 0.109375 C 1.203125 0.109375 1.375 0.03125 1.453125 -0.171875 C 1.46875 -0.203125 1.796875 -1.5625 1.84375 -1.75 C 1.921875 -2.078125 2.109375 -2.765625 2.15625 -3.046875 C 2.203125 -3.171875 2.484375 -3.640625 2.71875 -3.859375 C 2.796875 -3.921875 3.09375 -4.1875 3.515625 -4.1875 C 3.78125 -4.1875 3.921875 -4.0625 3.9375 -4.0625 C 3.640625 -4.015625 3.421875 -3.78125 3.421875 -3.515625 C 3.421875 -3.359375 3.53125 -3.171875 3.796875 -3.171875 C 4.0625 -3.171875 4.34375 -3.40625 4.34375 -3.765625 C 4.34375 -4.109375 4.03125 -4.40625 3.515625 -4.40625 C 2.875 -4.40625 2.4375 -3.921875 2.25 -3.640625 C 2.15625 -4.09375 1.796875 -4.40625 1.328125 -4.40625 C 0.875 -4.40625 0.6875 -4.015625 0.59375 -3.84375 C 0.421875 -3.5 0.296875 -2.90625 0.296875 -2.875 C 0.296875 -2.765625 0.390625 -2.765625 0.40625 -2.765625 C 0.515625 -2.765625 0.515625 -2.78125 0.578125 -3 C 0.75 -3.703125 0.953125 -4.1875 1.3125 -4.1875 C 1.46875 -4.1875 1.609375 -4.109375 1.609375 -3.734375 C 1.609375 -3.515625 1.578125 -3.40625 1.453125 -2.890625 Z M 0.875 -0.59375 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-10">
              <path
                style="stroke: none;"
                d="M 4.671875 -2.71875 C 4.671875 -3.765625 3.984375 -4.40625 3.078125 -4.40625 C 1.75 -4.40625 0.40625 -2.984375 0.40625 -1.578125 C 0.40625 -0.59375 1.078125 0.109375 2 0.109375 C 3.328125 0.109375 4.671875 -1.265625 4.671875 -2.71875 Z M 2.015625 -0.109375 C 1.578125 -0.109375 1.140625 -0.421875 1.140625 -1.203125 C 1.140625 -1.6875 1.40625 -2.765625 1.71875 -3.265625 C 2.21875 -4.03125 2.796875 -4.1875 3.078125 -4.1875 C 3.65625 -4.1875 3.953125 -3.703125 3.953125 -3.109375 C 3.953125 -2.71875 3.75 -1.671875 3.375 -1.03125 C 3.015625 -0.453125 2.46875 -0.109375 2.015625 -0.109375 Z M 2.015625 -0.109375 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-11">
              <path
                style="stroke: none;"
                d="M 0.453125 1.21875 C 0.375 1.5625 0.34375 1.625 -0.09375 1.625 C -0.203125 1.625 -0.3125 1.625 -0.3125 1.8125 C -0.3125 1.890625 -0.265625 1.9375 -0.1875 1.9375 C 0.078125 1.9375 0.375 1.90625 0.640625 1.90625 C 0.984375 1.90625 1.3125 1.9375 1.640625 1.9375 C 1.6875 1.9375 1.8125 1.9375 1.8125 1.734375 C 1.8125 1.625 1.71875 1.625 1.578125 1.625 C 1.078125 1.625 1.078125 1.5625 1.078125 1.46875 C 1.078125 1.34375 1.5 -0.28125 1.5625 -0.53125 C 1.6875 -0.234375 1.96875 0.109375 2.484375 0.109375 C 3.640625 0.109375 4.890625 -1.34375 4.890625 -2.8125 C 4.890625 -3.75 4.3125 -4.40625 3.5625 -4.40625 C 3.0625 -4.40625 2.578125 -4.046875 2.25 -3.65625 C 2.15625 -4.203125 1.71875 -4.40625 1.359375 -4.40625 C 0.890625 -4.40625 0.703125 -4.015625 0.625 -3.84375 C 0.4375 -3.5 0.3125 -2.90625 0.3125 -2.875 C 0.3125 -2.765625 0.40625 -2.765625 0.421875 -2.765625 C 0.53125 -2.765625 0.53125 -2.78125 0.59375 -3 C 0.765625 -3.703125 0.96875 -4.1875 1.328125 -4.1875 C 1.5 -4.1875 1.640625 -4.109375 1.640625 -3.734375 C 1.640625 -3.5 1.609375 -3.390625 1.5625 -3.21875 Z M 2.203125 -3.109375 C 2.265625 -3.375 2.546875 -3.65625 2.71875 -3.8125 C 3.078125 -4.109375 3.359375 -4.1875 3.53125 -4.1875 C 3.921875 -4.1875 4.171875 -3.84375 4.171875 -3.25 C 4.171875 -2.65625 3.84375 -1.515625 3.65625 -1.140625 C 3.3125 -0.4375 2.84375 -0.109375 2.46875 -0.109375 C 1.8125 -0.109375 1.6875 -0.9375 1.6875 -1 C 1.6875 -1.015625 1.6875 -1.03125 1.71875 -1.15625 Z M 2.203125 -3.109375 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-12">
              <path
                style="stroke: none;"
                d="M 2.828125 -6.234375 C 2.828125 -6.4375 2.6875 -6.59375 2.46875 -6.59375 C 2.1875 -6.59375 1.921875 -6.328125 1.921875 -6.0625 C 1.921875 -5.875 2.0625 -5.703125 2.296875 -5.703125 C 2.53125 -5.703125 2.828125 -5.9375 2.828125 -6.234375 Z M 2.078125 -2.484375 C 2.1875 -2.765625 2.1875 -2.796875 2.296875 -3.0625 C 2.375 -3.265625 2.421875 -3.40625 2.421875 -3.59375 C 2.421875 -4.03125 2.109375 -4.40625 1.609375 -4.40625 C 0.671875 -4.40625 0.296875 -2.953125 0.296875 -2.875 C 0.296875 -2.765625 0.390625 -2.765625 0.40625 -2.765625 C 0.515625 -2.765625 0.515625 -2.796875 0.5625 -2.953125 C 0.84375 -3.890625 1.234375 -4.1875 1.578125 -4.1875 C 1.65625 -4.1875 1.828125 -4.1875 1.828125 -3.875 C 1.828125 -3.65625 1.75 -3.453125 1.71875 -3.34375 C 1.640625 -3.09375 1.1875 -1.9375 1.03125 -1.5 C 0.921875 -1.25 0.796875 -0.921875 0.796875 -0.703125 C 0.796875 -0.234375 1.140625 0.109375 1.609375 0.109375 C 2.546875 0.109375 2.921875 -1.328125 2.921875 -1.421875 C 2.921875 -1.53125 2.828125 -1.53125 2.796875 -1.53125 C 2.703125 -1.53125 2.703125 -1.5 2.65625 -1.34375 C 2.46875 -0.71875 2.140625 -0.109375 1.640625 -0.109375 C 1.46875 -0.109375 1.390625 -0.203125 1.390625 -0.4375 C 1.390625 -0.6875 1.453125 -0.828125 1.6875 -1.4375 Z M 2.078125 -2.484375 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-13">
              <path
                style="stroke: none;"
                d="M 0.875 -0.59375 C 0.84375 -0.4375 0.78125 -0.203125 0.78125 -0.15625 C 0.78125 0.015625 0.921875 0.109375 1.078125 0.109375 C 1.203125 0.109375 1.375 0.03125 1.453125 -0.171875 C 1.453125 -0.1875 1.578125 -0.65625 1.640625 -0.90625 L 1.859375 -1.796875 C 1.90625 -2.03125 1.96875 -2.25 2.03125 -2.46875 C 2.0625 -2.640625 2.140625 -2.9375 2.15625 -2.96875 C 2.296875 -3.28125 2.828125 -4.1875 3.78125 -4.1875 C 4.234375 -4.1875 4.3125 -3.8125 4.3125 -3.484375 C 4.3125 -2.875 3.828125 -1.59375 3.671875 -1.171875 C 3.578125 -0.9375 3.5625 -0.8125 3.5625 -0.703125 C 3.5625 -0.234375 3.921875 0.109375 4.390625 0.109375 C 5.328125 0.109375 5.6875 -1.34375 5.6875 -1.421875 C 5.6875 -1.53125 5.609375 -1.53125 5.578125 -1.53125 C 5.46875 -1.53125 5.46875 -1.5 5.421875 -1.34375 C 5.21875 -0.671875 4.890625 -0.109375 4.40625 -0.109375 C 4.234375 -0.109375 4.171875 -0.203125 4.171875 -0.4375 C 4.171875 -0.6875 4.25 -0.921875 4.34375 -1.140625 C 4.53125 -1.671875 4.953125 -2.765625 4.953125 -3.34375 C 4.953125 -4 4.53125 -4.40625 3.8125 -4.40625 C 2.90625 -4.40625 2.421875 -3.765625 2.25 -3.53125 C 2.203125 -4.09375 1.796875 -4.40625 1.328125 -4.40625 C 0.875 -4.40625 0.6875 -4.015625 0.59375 -3.84375 C 0.421875 -3.5 0.296875 -2.90625 0.296875 -2.875 C 0.296875 -2.765625 0.390625 -2.765625 0.40625 -2.765625 C 0.515625 -2.765625 0.515625 -2.78125 0.578125 -3 C 0.75 -3.703125 0.953125 -4.1875 1.3125 -4.1875 C 1.5 -4.1875 1.609375 -4.0625 1.609375 -3.734375 C 1.609375 -3.515625 1.578125 -3.40625 1.453125 -2.890625 Z M 0.875 -0.59375 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-14">
              <path
                style="stroke: none;"
                d="M 2.046875 -3.984375 L 2.984375 -3.984375 C 3.1875 -3.984375 3.296875 -3.984375 3.296875 -4.1875 C 3.296875 -4.296875 3.1875 -4.296875 3.015625 -4.296875 L 2.140625 -4.296875 C 2.5 -5.71875 2.546875 -5.90625 2.546875 -5.96875 C 2.546875 -6.140625 2.421875 -6.234375 2.25 -6.234375 C 2.21875 -6.234375 1.9375 -6.234375 1.859375 -5.875 L 1.46875 -4.296875 L 0.53125 -4.296875 C 0.328125 -4.296875 0.234375 -4.296875 0.234375 -4.109375 C 0.234375 -3.984375 0.3125 -3.984375 0.515625 -3.984375 L 1.390625 -3.984375 C 0.671875 -1.15625 0.625 -0.984375 0.625 -0.8125 C 0.625 -0.265625 1 0.109375 1.546875 0.109375 C 2.5625 0.109375 3.125 -1.34375 3.125 -1.421875 C 3.125 -1.53125 3.046875 -1.53125 3.015625 -1.53125 C 2.921875 -1.53125 2.90625 -1.5 2.859375 -1.390625 C 2.4375 -0.34375 1.90625 -0.109375 1.5625 -0.109375 C 1.359375 -0.109375 1.25 -0.234375 1.25 -0.5625 C 1.25 -0.8125 1.28125 -0.875 1.3125 -1.046875 Z M 2.046875 -3.984375 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-15">
              <path
                style="stroke: none;"
                d="M 3.484375 -0.5625 C 3.59375 -0.15625 3.953125 0.109375 4.375 0.109375 C 4.71875 0.109375 4.953125 -0.125 5.109375 -0.4375 C 5.28125 -0.796875 5.40625 -1.40625 5.40625 -1.421875 C 5.40625 -1.53125 5.328125 -1.53125 5.296875 -1.53125 C 5.1875 -1.53125 5.1875 -1.484375 5.15625 -1.34375 C 5.015625 -0.78125 4.828125 -0.109375 4.40625 -0.109375 C 4.203125 -0.109375 4.09375 -0.234375 4.09375 -0.5625 C 4.09375 -0.78125 4.21875 -1.25 4.296875 -1.609375 L 4.578125 -2.6875 C 4.609375 -2.828125 4.703125 -3.203125 4.75 -3.359375 C 4.796875 -3.59375 4.890625 -3.96875 4.890625 -4.03125 C 4.890625 -4.203125 4.75 -4.296875 4.609375 -4.296875 C 4.5625 -4.296875 4.296875 -4.28125 4.21875 -3.953125 C 4.03125 -3.21875 3.59375 -1.46875 3.46875 -0.953125 C 3.453125 -0.90625 3.0625 -0.109375 2.328125 -0.109375 C 1.8125 -0.109375 1.71875 -0.5625 1.71875 -0.921875 C 1.71875 -1.484375 2 -2.265625 2.25 -2.953125 C 2.375 -3.265625 2.421875 -3.40625 2.421875 -3.59375 C 2.421875 -4.03125 2.109375 -4.40625 1.609375 -4.40625 C 0.65625 -4.40625 0.296875 -2.953125 0.296875 -2.875 C 0.296875 -2.765625 0.390625 -2.765625 0.40625 -2.765625 C 0.515625 -2.765625 0.515625 -2.796875 0.5625 -2.953125 C 0.8125 -3.8125 1.203125 -4.1875 1.578125 -4.1875 C 1.671875 -4.1875 1.828125 -4.171875 1.828125 -3.859375 C 1.828125 -3.625 1.71875 -3.328125 1.65625 -3.1875 C 1.28125 -2.1875 1.078125 -1.578125 1.078125 -1.09375 C 1.078125 -0.140625 1.765625 0.109375 2.296875 0.109375 C 2.953125 0.109375 3.3125 -0.34375 3.484375 -0.5625 Z M 3.484375 -0.5625 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph0-16">
              <path
                style="stroke: none;"
                d="M 5.140625 -6.8125 C 5.140625 -6.8125 5.140625 -6.921875 5.015625 -6.921875 C 4.859375 -6.921875 3.921875 -6.828125 3.75 -6.8125 C 3.671875 -6.796875 3.609375 -6.75 3.609375 -6.625 C 3.609375 -6.5 3.703125 -6.5 3.84375 -6.5 C 4.328125 -6.5 4.34375 -6.4375 4.34375 -6.328125 L 4.3125 -6.125 L 3.71875 -3.765625 C 3.53125 -4.140625 3.25 -4.40625 2.796875 -4.40625 C 1.640625 -4.40625 0.40625 -2.9375 0.40625 -1.484375 C 0.40625 -0.546875 0.953125 0.109375 1.71875 0.109375 C 1.921875 0.109375 2.421875 0.0625 3.015625 -0.640625 C 3.09375 -0.21875 3.453125 0.109375 3.921875 0.109375 C 4.28125 0.109375 4.5 -0.125 4.671875 -0.4375 C 4.828125 -0.796875 4.96875 -1.40625 4.96875 -1.421875 C 4.96875 -1.53125 4.875 -1.53125 4.84375 -1.53125 C 4.75 -1.53125 4.734375 -1.484375 4.703125 -1.34375 C 4.53125 -0.703125 4.359375 -0.109375 3.953125 -0.109375 C 3.671875 -0.109375 3.65625 -0.375 3.65625 -0.5625 C 3.65625 -0.8125 3.671875 -0.875 3.703125 -1.046875 Z M 3.078125 -1.1875 C 3.015625 -1 3.015625 -0.984375 2.875 -0.8125 C 2.4375 -0.265625 2.03125 -0.109375 1.75 -0.109375 C 1.25 -0.109375 1.109375 -0.65625 1.109375 -1.046875 C 1.109375 -1.546875 1.421875 -2.765625 1.65625 -3.234375 C 1.96875 -3.8125 2.40625 -4.1875 2.8125 -4.1875 C 3.453125 -4.1875 3.59375 -3.375 3.59375 -3.3125 C 3.59375 -3.25 3.578125 -3.1875 3.5625 -3.140625 Z M 3.078125 -1.1875 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph1-0">
              <path
                style="stroke: none;"
                d="M 3.296875 2.390625 C 3.296875 2.359375 3.296875 2.34375 3.125 2.171875 C 1.890625 0.921875 1.5625 -0.96875 1.5625 -2.5 C 1.5625 -4.234375 1.9375 -5.96875 3.171875 -7.203125 C 3.296875 -7.328125 3.296875 -7.34375 3.296875 -7.375 C 3.296875 -7.453125 3.265625 -7.484375 3.203125 -7.484375 C 3.09375 -7.484375 2.203125 -6.796875 1.609375 -5.53125 C 1.109375 -4.4375 0.984375 -3.328125 0.984375 -2.5 C 0.984375 -1.71875 1.09375 -0.515625 1.640625 0.625 C 2.25 1.84375 3.09375 2.5 3.203125 2.5 C 3.265625 2.5 3.296875 2.46875 3.296875 2.390625 Z M 3.296875 2.390625 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph1-1">
              <path
                style="stroke: none;"
                d="M 2.875 -2.5 C 2.875 -3.265625 2.765625 -4.46875 2.21875 -5.609375 C 1.625 -6.828125 0.765625 -7.484375 0.671875 -7.484375 C 0.609375 -7.484375 0.5625 -7.4375 0.5625 -7.375 C 0.5625 -7.34375 0.5625 -7.328125 0.75 -7.140625 C 1.734375 -6.15625 2.296875 -4.578125 2.296875 -2.5 C 2.296875 -0.78125 1.9375 0.96875 0.703125 2.21875 C 0.5625 2.34375 0.5625 2.359375 0.5625 2.390625 C 0.5625 2.453125 0.609375 2.5 0.671875 2.5 C 0.765625 2.5 1.671875 1.8125 2.25 0.546875 C 2.765625 -0.546875 2.875 -1.65625 2.875 -2.5 Z M 2.875 -2.5 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph1-2">
              <path
                style="stroke: none;"
                d="M 6.84375 -3.265625 C 7 -3.265625 7.1875 -3.265625 7.1875 -3.453125 C 7.1875 -3.65625 7 -3.65625 6.859375 -3.65625 L 0.890625 -3.65625 C 0.75 -3.65625 0.5625 -3.65625 0.5625 -3.453125 C 0.5625 -3.265625 0.75 -3.265625 0.890625 -3.265625 Z M 6.859375 -1.328125 C 7 -1.328125 7.1875 -1.328125 7.1875 -1.53125 C 7.1875 -1.71875 7 -1.71875 6.84375 -1.71875 L 0.890625 -1.71875 C 0.75 -1.71875 0.5625 -1.71875 0.5625 -1.53125 C 0.5625 -1.328125 0.75 -1.328125 0.890625 -1.328125 Z M 6.859375 -1.328125 "
              />
            </symbol>
            <symbol overflow="visible" id="glyph1-3">
              <path
                style="stroke: none;"
                d="M 4.078125 -2.296875 L 6.859375 -2.296875 C 7 -2.296875 7.1875 -2.296875 7.1875 -2.5 C 7.1875 -2.6875 7 -2.6875 6.859375 -2.6875 L 4.078125 -2.6875 L 4.078125 -5.484375 C 4.078125 -5.625 4.078125 -5.8125 3.875 -5.8125 C 3.671875 -5.8125 3.671875 -5.625 3.671875 -5.484375 L 3.671875 -2.6875 L 0.890625 -2.6875 C 0.75 -2.6875 0.5625 -2.6875 0.5625 -2.5 C 0.5625 -2.296875 0.75 -2.296875 0.890625 -2.296875 L 3.671875 -2.296875 L 3.671875 0.5 C 3.671875 0.640625 3.671875 0.828125 3.875 0.828125 C 4.078125 0.828125 4.078125 0.640625 4.078125 0.5 Z M 4.078125 -2.296875 "
              />
            </symbol>
          </g>
        </defs>
        <g id="surface1">
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-0" x="-0.3" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph1-0" x="7.575" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-1" x="11.44492" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-2" x="17.14192" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-3" x="21.560939" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-4" x="26.231938" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-5" x="30.542937" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-6" x="35.807941" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-7" x="38.975786" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-2" x="43.610787" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-8" x="48.029696" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-7" x="53.105867" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-9" x="57.740863" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-10" x="62.510942" y="11.68086" />
          </g>
          <path
            style="
              fill: none;
              stroke-width: 4.05;
              stroke-linecap: butt;
              stroke-linejoin: miter;
              stroke: rgb(0%, 0%, 0%);
              stroke-opacity: 1;
              stroke-miterlimit: 10;
            "
            d="M 679.375 75.195312 L 709.335938 75.195312 "
            transform="matrix(0.1,0,0,-0.1,0,19)"
          />
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-11" x="70.9531" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-10" x="75.966102" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-12" x="80.799104" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-13" x="84.228102" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-14" x="90.204099" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph1-1" x="93.8043" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph1-2" x="100.446492" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-9" x="110.9586" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-10" x="115.728521" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-15" x="120.561523" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-13" x="126.267526" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-16" x="132.243523" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph1-0" x="137.4274" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-1" x="150.684" y="4.93984" />
          </g>
          <path
            style="
              fill: none;
              stroke-width: 4.05;
              stroke-linecap: butt;
              stroke-linejoin: miter;
              stroke: rgb(0%, 0%, 0%);
              stroke-opacity: 1;
              stroke-miterlimit: 10;
            "
            d="M 1424.921875 98.085938 L 1645.546875 98.085938 "
            transform="matrix(0.1,0,0,-0.1,0,19)"
          />
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-3" x="142.494" y="18.512109" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-4" x="147.164999" y="18.512109" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-5" x="151.476001" y="18.512109" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-6" x="156.740999" y="18.512109" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-7" x="159.908841" y="18.512109" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph1-3" x="167.9639" y="11.680859" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-8" x="177.92679" y="11.680859" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-7" x="183.00296" y="11.680859" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-9" x="187.637956" y="11.680859" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-10" x="192.408035" y="11.680859" />
          </g>
          <path
            style="
              fill: none;
              stroke-width: 4.05;
              stroke-linecap: butt;
              stroke-linejoin: miter;
              stroke: rgb(0%, 0%, 0%);
              stroke-opacity: 1;
              stroke-miterlimit: 10;
            "
            d="M 1978.359375 75.195312 L 2008.320312 75.195312 "
            transform="matrix(0.1,0,0,-0.1,0,19)"
          />
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-11" x="200.823" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-10" x="205.83599" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-12" x="210.668992" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-13" x="214.097996" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph0-14" x="220.073993" y="11.68086" />
          </g>
          <g style="fill: rgb(0%, 0%, 0%); fill-opacity: 1;">
            <use xlink:href="#glyph1-1" x="223.6742" y="11.68086" />
          </g>
        </g></svg></span
    >, что несколько напоминает преобразование в батч-нормализации. Учитывая это и некоторые другие факторы, при
    статической квантизации применяется слияние (fusing) Conv-модулей с BN-модулями и с ReLU в один модуль ConvBNReLU.
    Аналогичным образом поступаем и с Linear-модулями.
  </li>
  <li>
    Определение конфигурации метода квантизации. Я органичился значениями по умолчанию, получаемыми командой
    <span>torch.quantization.get_default_qconfig('fbgemm')</span>
  </li>
  <li>
    "Подготовка" (prepare) модели. Статическая квантизация предполагает преобразование тензоров согласно выражению выше;
    для определения
    <span>scale</span> и <span>zero_point</span> PyTorch использует так называемые <span>observers</span> (наблюдатели),
    которые оценивают распределение данных после каждой активационной функции, тем самым опытным путем определяют
    параметры <span>scale</span> и <span>zero_point</span>.
  </li>
  <li>
    Калибровка. Для того, чтобы "наблюдатели" собрали необходимые данные, необходимо хотя бы один раз прогнать реальные
    изображения через модель.
  </li>
  <li>
    Конвертация. В завершении осуществляется непосредственно применение полученных в предыдущих пунктах
    <span>scale</span> и <span>zero_point</span> к модели, после чего модель можно считать полностью квантизированной и
    готовой к работе.
  </li>
</ol>
<p>
  Рассматривая предварительно тренированную на датасете COCO2017 модель MaskRCNN с ResNet50 в качестве бэкбона из
  PyTorch <span>torchvision</span> моделей, я тут же столкнулся с тем, что на текущий момент (версия torchvision 0.5.0),
  эта модель не поддерживает квантизация прямо "из коробки". Поэтому я немного модифицировал ее. Наиболее "затратные" по
  памяти части MaskRCNN это backbone-FPN-ResNet50(~107Мб) и box_head(~55Мб), поэтому я квантизировал только их, оставив
  "как есть" все остальное. Полный код исходной модели можно посмотреть в оригинальном
  <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/detection/mask_rcnn.py">репозитории</a>
  PyTorch, на изучение которого, с учетом модульности кода, у меня ушел не один час.
</p>
<p>
  В рассматриваемой MaskRCNN в качестве backbone используется
  <a href="https://arxiv.org/pdf/1612.03144.pdf">Feature Pyramid Networks (FPN)</a>
  на основе ResNet50. Как видно из
  <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/detection/backbone_utils.py">кода</a>,
  оригинальная
  <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py">ResNet</a>
  трансформируется в новую модель с помощью
  <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/_utils.py">IntermediateLayerGetter</a>
  для вывода промежуточных данных из слоев (Layer1-Layer4), которые и используются FPN.
</p>
<p>
  Torchvision имеет готовую квантизированную
  <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/quantization/resnet.py">ResNet50</a>, ее
  беру за основу, трансформирую <span>IntermediateLayerGetter</span>, не забывая добавить функцию
  <span>fuse_model</span> для дальнейшего слияния модулей. После чего, на этой основе инициирую новый backbone
  <span>BackBoneWithFPN</span>. Квантизация входных данных будет происходить до самого первого модуля ResNet, а
  деквантизация - после самого последнего модуля FPN. Реализация вышеописанного может быть примерно такая:
</p>
<div class="code-spoiler"></div>
<pre>
    from collections import OrderedDict
    import torch
    import torch.nn.functional as F
    from torch import nn, Tensor
    from torch.jit.annotations import Tuple, List, Dict

    # Берем за основу существующий в PyTorch класс для извлечения промежуточных данных
    class IntermediateLayerGetterQuant(torchvision.models._utils.IntermediateLayerGetter):
        def __init__(self, model,
                     return_layers={'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}):
            if not set(return_layers).issubset([name for name, _ in model.named_children()]):
                raise ValueError("return_layers are not present in model")
            super(IntermediateLayerGetterQuant, self).__init__(model, return_layers)

        # Определяем функцию слияния входящих в модуль подмодулей
        def fuse_model(self):
            torch.quantization.fuse_modules(self, ['conv1', 'bn1', 'relu'], inplace=True)
            for m in self.modules():
                if type(m) == torchvision.models.quantization.resnet.QuantizableBottleneck or \
                   type(m) == torchvision.models.quantization.resnet.QuantizableBasicBlock:
                    m.fuse_model()

    # Практически полностью переопределяем FPN
    class FeaturePyramidNetworkQuant(nn.Module):
        __constants__ = ['inner_blocks', 'layer_blocks']

        def __init__(self, in_channels_list, out_channels, extra_blocks=None):
            super(FeaturePyramidNetworkQuant, self).__init__()
            self.inner_blocks = nn.ModuleList()
            self.layer_blocks = nn.ModuleList()
            self.FPN_add = nn.quantized.FloatFunctional()
            for in_channels in in_channels_list:
                if in_channels == 0:
                    continue
                inner_block_module = nn.Conv2d(in_channels, out_channels, 1)
                layer_block_module = nn.Conv2d(out_channels, out_channels, 3, padding=1)
                self.inner_blocks.append(inner_block_module)
                self.layer_blocks.append(layer_block_module)

        def forward(self, x:Dict[str, torch.Tensor]):
            names = list(x.keys())
            x = list(x.values())
            in_results = []
            i = 0
            for mod in self.inner_blocks:
                in_results.append(mod(x[i]))
                i += 1
            l_results = [in_results[-1]]
            i = len(in_results) - 2
            while i >= 0:
                feat_shape = in_results[i].shape[-2:]
                inner_top_down = torch.nn.functional.interpolate(l_results[0], size=feat_shape, mode="nearest")
                l_results.insert(0, self.FPN_add.add(in_results[i], inner_top_down))
                i -= 1
            i = 0
            results = []
            for mod in self.layer_blocks:
                results.append(mod(l_results[i]))
                i += 1

            # Преобразовываем в OrderedDict как в оригинале
            out = OrderedDict([(k, v) for k, v in zip(names, results)])
            out['pool'] = torch.nn.functional.max_pool2d(results[-1], 1, 2, 0)
            return out


    class BackboneWithFPN(nn.Module):
        def __init__(self, backbone, return_layers, in_channels_list, out_channels):
            super().__init__()
            self.body = IntermediateLayerGetterQuant(backbone, return_layers=return_layers)
            self.fpn = FeaturePyramidNetworkQuant(
                in_channels_list=in_channels_list,
                out_channels=out_channels,
            )
            self.quant = torch.quantization.QuantStub()
            self.dequant = torch.quantization.DeQuantStub()
            self.out_channels = out_channels

        def forward(self, input):
            out = self.fpn(self.body(self.quant(input)))
            out = OrderedDict([(k, self.dequant(v)) for k, v in out.items()])
            return out

        def fuse_model(self):            
            for m in self.modules():
                if type(m) == IntermediateLayerGetterQuant:
                    m.fuse_model()


    def resnet50_fpn_backbone(backbone=None, pretrained=False):
        if backbone is None:
            # Используем готовую к квантизированию ResNet50 из torchvision без весов
            backbone = torchvision.models.quantization.resnet50(pretrained=False, quantize=False, norm_layer=torch.nn.BatchNorm2d)

        return_layers = {'layer1': '0', 'layer2': '1', 'layer3': '2', 'layer4': '3'}

        in_channels_stage2 = backbone.inplanes // 8
        in_channels_list = [
            in_channels_stage2,
            in_channels_stage2 * 2,
            in_channels_stage2 * 4,
            in_channels_stage2 * 8,
        ]
        out_channels = 256
        return BackboneWithFPN(backbone, return_layers, in_channels_list, out_channels)</pre
>

<p>
  Таким образом, функция <span>resnet50_fpn_backbone</span> возвращает backbone, полностью идентичный бэкбону MaskRCNN
  из <span>torchvision</span>. Подобным образом я переопределил и модуль box_head, прототипом которого является класс
  TwoMLPHead, который можно найти в
  <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/detection/faster_rcnn.py">этом файле</a>
  репозитория PyTorch. Добавляя квантизирующий и деквантизрующий модули, а также функцию слияния, получаю примерно
  следующий код:
</p>
<div class="code-spoiler"></div>
<pre>
    class TwoMLPHead_quantized(nn.Module):
   
        def __init__(self, in_channels, representation_size):
            super(TwoMLPHead_quantized, self).__init__()
    
            self.fc6 = nn.Linear(in_channels, representation_size)
            self.fc7 = nn.Linear(representation_size, representation_size)
            self.relu6 = nn.ReLU(inplace=False)
            self.relu7 = nn.ReLU(inplace=False)
            self.quant = torch.quantization.QuantStub()
            self.dequant = torch.quantization.DeQuantStub()
    
        def forward(self, x):
            x = x.flatten(start_dim=1)
            x = self.quant(x)
            x = self.fc6(x)
            x = self.relu6(x)
            x = self.fc7(x)
            x = self.relu7(x)
            x = self.dequant(x)
    
            return x
    
        def fuse_model(self):
            torch.quantization.fuse_modules(self, [['fc6', 'relu6'],
                                                   ['fc7', 'relu7']], inplace=True)</pre
>
<p>
  Теперь остается только подменить модули оригинальной MaskRCNN и загрузить веса:
</p>
<div class="code-spoiler"></div>
<pre>
    orig_model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)
    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)
    model.backbone = resnet50_fpn_backbone()
    model.roi_heads.box_head = TwoMLPHead_quantized(12544, 1024)
    model.load_state_dict(orig_model.state_dict())
</pre>
<p>
  Первый (согласно вышеуказанному списку) этап квантизации модели окончен. Теперь новая MaskRCNN готова к следующим
  этапам, которые уже, практически, буду проходить в автоматическом режиме и реализуются следующим кодом.
</p>
<div class="code-spoiler"></div>
<pre>
    from PIL import Image
    from io import BytesIO
    import requests

    model.eval()
    # Определение конфигурации
    model.backbone.qconfig = torch.quantization.get_default_qconfig('fbgemm')
    model.roi_heads.box_head.qconfig = torch.quantization.get_default_qconfig('fbgemm')

    # Слияние модулей
    model.backbone.fuse_model()
    model.roi_heads.box_head.fuse_model()
    torch.quantization.prepare(model, inplace=True)

    # Калибровка. Обеспечиваем хотя бы одно входное изображение
    # Лучше выполнять калибровку более чем на одном изображении
    url = 'https://www.sb.by/upload/iblock/e26/e2695b04032819905a3150f90ef27dd9.jpg'
    image = torchvision.transforms.ToTensor()(
        Image.open(BytesIO(requests.get(url).content)))
    with torch.no_grad():
        model([image])

    # Конвертация
    torch.quantization.convert(model, inplace=True)</pre
>
<p>
  В итоге получилась MaskRCNN, в которой квантизированы наиболее ресорсоемкие части. Мы можем подавать на нее тензоры
  изображений (как и в обычной PyTorch MaskRCNN, - массив тензоров) и получать вполне удовлетворительный результат.
  Остается открытым вопрос, как сохранить эту модель для дальнейшего использования. Для сохранения модели пока есть
  только одна возможность, - воспользуемся
  <a href="https://pytorch.org/docs/stable/jit.html">TorchScript</a>. С первого взгляда этот инструмент переворачивает
  все представление о гибкости Python-а, так как, помимо прочего, он предназначен для использования PyTorch-моделей на
  C++, соответственно, здесь присутствует жесткая типизация данных, что накладывает весьма существенные органичения на
  создание PyTorch-моделей с последующей конвертацией в TorchScript-модели, а конвертация уже существующих
  PyTorch-моделей может доставить новичку весьма ощутимую "боль". Более подробно с TorchScript можно ознакомиться в
  <a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html">туториале</a>, а также на
  <a href="https://habr.com/ru/company/ods/blog/480328/">хабре</a>. К счастью, в torchvision, начиная с версии 0.5.0,
  имеющаяся MaskRCNN уже приспособлена для такой конвертации, а вышеприведенные действия по квантизации этой модели я
  выполнял уже с учетом требований TorchScript, таким образом можно сохранить квантизированную модель простой командой
</p>
<pre>
   torch.jit.save(torch.jit.script(model), 'quantized_model.pth')
</pre>
<p>
  Прелесть TorchScript в том, что сохраняются не только веса, а модель полностью. Таким образом, можно загрузить ее
  командой
</p>
<pre>   model = torch.jit.load('quantized_model.pth')</pre>
<p style="text-indent: 0;">
  и сразу же использовать. Правда, как оказалось, есть один нюанс, который явно не описан в документации, первые 1-2
  "прогона" загруженная TorchScript-модель оптимизируется, и время этой оптимизации может быть достаточно велико.
  Размещая подобную модель в flask-приложении необходимо это учитывать. Выхода я пока нашел два. Отключить эту
  оптимизацию с помощью
  <span>torch.jit.optimized_execution(False)</span>, либо использовать модель как глобальную переменную python-а,
  инициализируя и прогоняя для оптимизации 1-2 раза непосредственно при запуске сервера.
</p>
<p>
  Сохраненная квантизированная модель MaskRCNN занимает порядка 54Мб против 177Мб в случае не квантизированной. После
  загрузки в память квантизированная MaskRCNN занимает в 3 раза меньше объема (~105Мб против ~346Мб); увеличение расхода
  памяти после обработки одного среднестатистического изображения меньше в ~2.5 раза (~715Мб против ~1785Мб). Среднее
  время выполнения на CPU (Inference-time) уменьшилось всего на ~23%, что я объяснил тем, что рассматриваемая модель
  производит достаточно много вычислений в Region Proposal Network, что никак не оптимизируется квантизацией. Весь
  процесс квантизирования я производил в Colab-е в этом
  <a href="https://colab.research.google.com/github/tuuka/cv-demo/blob/master/Quantize%26Script_MaskRCNN.ipynb"
    >jupyter-notebook</a
  >.
</p>
<p>
  Я пока не обладаю достаточными умениями в С++, чтобы в полной мере оценить работу заскриптованной модели без
  необходимости установки "тяжелого" PyTorch. Но, для эксперимента и разнообразия, была осуществлена более-менее
  успешная попытка разместить полученную квантизированную модель на <span>Heroku</span> и на
  <span>Google App Engine Standart</span> с простейшим подобием API, чтобы можно было обращаться из
  основного приложения. <span>Heroku</span> имеет ARM-процессор, соответственно, необходимо использовать модель с
  QNNPACK-бэкэендом; GAE работает с обоими типами бэкэндов, поддерживаемых (на текущий момент) PyTorch.
</p>
<p>
  Эта реализация все еще является предметом моих активных экспериментов в области борьбы с
  <span>memory leak</span>, в текущем состоянии она доступна <a href="https://github.com/tuuka/markrcnn">здесь</a>.
  Процесс размещения на <span>Heroku</span> такой же, как в книге
  <a href="https://blog.miguelgrinberg.com/post/the-flask-mega-tutorial-part-xviii-deployment-on-heroku">Мигуэля Гринберга</a>, а
  размещение на <span>GAE</span> описано ранее.
</p>
<p>
  Так как оба сервиса предоставляют прямую url-ссылку, подключить размещенную таким образом модель в основное приложение
  не составляет особого труда. Это можно сделать и непосредственно из JavaScript-а, и из flask-route, вставив
  дополнительную проверку передаваемого имени используемой модели.
</p>

