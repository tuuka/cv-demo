{% extends "base.html" %}

{% block app_content %}

<div class="">

    <!-- Modal page info -->
    <div class="modal fade" id="PageModalDescription" tabindex="-1" role="dialog">
    <div class="modal-dialog modal-xl modal-dialog-centered" role="document">
        <div class="modal-content">
          <div class="modal-header">
            <p class="modal-title" style="font-size: calc(12px + 1.1vw); font-weight: bold">{{ _('Semantic segmentation') }}</p>
            <button type="button" class="close" data-dismiss="modal" aria-label="Close">
              <span aria-hidden="true">&times;</span>
            </button>
          </div>
          <div class="modal-body text-justify" style="font-size: 0.8rem; line-height: 1.1; font-family: Arial">
              <p class="">{{ _('Semantic Segmentation is a classic ') }}
                  <a href="https://en.wikipedia.org/wiki/Computer_vision">{{ _('Computer Vision') }}</a>
                  {{ _(' technique used to understand what is in a given image at a pixel level. It is different than image recognition, which assigns one or more labels to an entire image; and object detection, which locatalizes objects within an image by drawing a bounding box around them. Image segmentation provides more fine-grain information about the contents of an image.') }}
                  {{ _('Semantic segmentation refers to the process of linking each pixel in an image to a class label. These labels could include a person, car, flower, piece of furniture, etc. Segmenting an image allows us to separate the foreground from background, identify the precise location of a bus or a building, and clearly mark the boundaries that separate a tree from the sky. Applications for semantic segmentation include autonomous driving, industrial inspection, classification of terrain visible in satellite imagery, medical imaging analysis and so on.') }}
              </p>
              <p> {{ _('Almost all modern models of artificial neural networks for semantic segmentation task are essentially ') }}
                  <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">{{ _('convolutional networks') }}</a>
                  {{ _('. And they consist of two main parts: an encoder and a decoder (or classifier in some cases). An encoder is, with some modifications, the usual convolutional neural network model for image recognition without the final pooling and classification layers. While decoders are more significantly different from model to model.') }}
              </p>
              <p> {{ _("Semantic segmentation is a huge section of computer vision that is still developing and has a huge number of different models, each of which has its own advantages and disadvantages. On this page we will consider a little part of semantic segmentation model's family trained on a subset of ") }}
                  <a href="http://cocodataset.org/#home">{{ _('COCO 2017') }}</a>
                  {{ _(' on the 20 categories that are present in the Pascal VOC dataset and, as experiment, few models trained on ') }}
                  <a href="https://groups.csail.mit.edu/vision/datasets/ADE20K/">{{ _('MIT ADE20K') }}</a>
                  {{ _('  dataset.') }}
              </p>
              <p> {{ _('On this page there is an area for previewing the image, to the right of which is the area which shows the 10 most common objects in the image, according to the used model. You can try to click on a prediction bars to online translate it into your local language (powered by Yandex translate). You can upload your image with "Browse" button or paste the image from the clipboard. You can also switch the models of neural networks using the buttons in the bottom. The prediction results will change automatically.') }}
              </p>
              <p style="color: red; font-weight: bold"> {{ _('Please note that this demo site is not hosted on a high-performance computer, so it will take some time to operate, be patient.') }}
              </p>
          </div>
        </div>
    </div>
    </div>
    <!-- /Modal page info -->

    <!-- Brief page description on top-->
    <div class="card" style="padding: 0; margin: 5px 0 0 0; ">
        <div class="card-body" style="padding: 5px 5px 0 5px; margin: 5px 5px -5px 5px;">
<!--        <button class="btn btn-primary fa fa-question-circle fa-siz fa-2x" id="page-help" style="float: right; padding: 0; margin: 0px 0px 3px 10px;"> </button>  -->
        <h6 class="text-justify" id="description-text" style="font-family: Yanone Kaffeesatz; font-size: calc(10px + 0.5vw) ;cursor: pointer;  line-height: 0.95;">
            {{ _('Select a file or paste an image from the clipboard or make a foto with a camera and an appropriate neural network model will try to make its semantic analysis. You can switch between models using the corresponding buttons below. ') }}
            <span style="color: red">{{ _('You can click this message for more details.') }} </span>
        </h6>
        </div>
    </div>
    <!-- /Brief page description on top-->

    {% include 'input_form.html' %}

    <!-- Preview and predict form -->
    <div class="d-flex flex-row justify-content-between preview-predict-form" style="">
        <div class="block-input-modal" style="position: absolute; width: 100%; height: 100%">
         <div style="position: absolute; top: 50%; left: 50%; transform: translateX(-50%) translateY(-50%); font-size: 3vw; font-weight: 600 ;">
             <span class="spinner-grow spinner-grow-lg" style="vertical-align: middle" role="status" aria-hidden="true"></span>
             <span class="wait-text-blinking">{{ _('Please wait') }}</span>
         </div>
        </div>
        <!-- Image preview and predicted image -->
        <div class="" style="height: 100%; width: 70%; margin-right: 0">
            <div class="image-in-preview">
                <img class="rounded" src="" id="imgpreview" alt="" style="object-fit: contain; height: 100%; width: 100%; z-index: 0;">
            </div>
            <div class="image-in-preview">
                <img class="rounded" src="" id="img_seg" alt="" style="object-fit: contain; height: 100%; width: 100%; z-index: 1; opacity: 0.6;">
            </div>
        </div>
        <!-- Prediction bars  -->
        <div style="width: 30%; margin-left: 1%;">
            <div class="" id="class-bars" style="position: absolute; overflow-y: auto; overflow-x: auto">
            <!--    <div class="progress-bar pred-item" role="progressbar" style="background-color: green;"></div>
            -->
            </div>
        </div>
    </div>
    <!-- /Preview and predict form -->

    <!-- Model change form -->
    <div class="d-block" style="position: relative;">
        <div class="f-flex block-input-modal" style=" "></div> <!-- Block models change -->
        <ul class="nav nav-pills" id="models-tab" role="tablist">
            <li class="nav-item">
                <a class="nav-link active" id="model1-tab" data-target="#model1-href" data-toggle="pill" href="./model?model=resnet101_fcn_coco20"  role="tab" aria-controls="model1-href" aria-selected="true">ResNetFCN</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" id="model2-tab" data-target="#model2-href" data-toggle="pill" href="./model?model=resnet101_deeplab_coco20" role="tab" aria-controls="model2-href" aria-selected="false">ResNetDeepLab</a>
            </li>
<!--            <li class="nav-item">-->
<!--                <a class="nav-link" id="model3-tab" data-target="#model3-href" data-toggle="pill" href="./model?model=resnet101_deeplab_ade20" role="tab" aria-controls="model3-href" aria-selected="false">ResNetDeepLab(ADE20K)</a>-->
<!--            </li>-->
<!--            <li class="nav-item">-->
<!--                <a class="nav-link" id="model4-tab" data-target="#model4-href" data-toggle="pill" href="./model?model=HRNET_ade20" role="tab" aria-controls="model4-href" aria-selected="false">HRNET(ADE20K)</a>-->
<!--            </li>-->
        </ul>
        <div class="tab-content models-tab" id="models-tabContent" style="">
            <!-- ResNet+FCN  -->
            <div class="tab-pane fade show active" id="model1-href" role="tabpanel" aria-labelledby="model1-tab">
            <div class="card">
                <div class="card-header" style="">
                    {{ _('Model is pretrained on COCO pascal VOC dataset (20 classes).') }}
                </div>
                <div class="card-body">
                    <p class="card-text">
                    {{ _('ResNet101+FCN is one of the simplest networks for semantic segmentation of images. It consists, like many others, of two main modules: an encoder and a decoder (sometimes called a backbone and a classifier respectively). The encoder of this model is a well-known model for image recognition ') }}
                    <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a>
                    {{ _(' (here in the variation of 101 layers) with dilated convolution in some layers and without the last pooling and classification fully connected layers. As a decoder, a fully convolutional network') }}
                    <a href="https://arxiv.org/pdf/1605.06211v1.pdf">FCN</a>
                    {{ _(' is used which consists of several (in this implementation one) 3x3 kernel convolutional layers with batch normalization and relu activation, followed by a convolutional layer with a 1x1 core to match the number of channels (feature maps) to the number of classification classes (labels ).') }}
                    </p>
                    <p>{{ _('During the training of such a model, additional classifiers are sometimes used (in some sources this technique is called ') }}
                    <a href="https://arxiv.org/pdf/1505.02496.pdf">Deep Supervision</a>
                    {{ _('). In this case, additional outputs from the intermediate layers of the encoder are fed to separate FCN classifiers. The loss function in this case is the weighted sum of the losses of each of these classifiers. ') }}
                    </p>
                    <p>
                    {{ _('Typically, the decoder outputs have a dimension less than that of the input image. To match the dimension of the output semantic mask with the size of the input image so-called transposed convolution is used, but in many cases simple bilinear upsampling is enough for these purposes.') }}
                    </p>
                    <p>{{ _('You can read more about FCN type models for semantic segmentation ') }}
                    <a href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1">{{ _('here') }}</a>
                    .
                    </p>
                </div>
            </div>
            </div>
            <!-- ResNet+Deeplabv3  -->
            <div class="tab-pane fade" id="model2-href" role="tabpanel" aria-labelledby="model2-tab">
            <div class="card">
                <div class="card-header" style="">
                    {{ _('Model is pretrained on COCO pascal VOC dataset (20 classes).') }}
                </div>
                <div class="card-body">
                    <p class="card-text">
                    {{ _('ResNet101+DeepLab is another example of a network for semantic segmentation of images. It consists, like many others, of two main modules: an encoder and a decoder (sometimes called a backbone and a classifier respectively). The encoder of this model, as you can see from the its caption, is also well-known model for image recognition ') }}
                    <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet101</a>
                    {{ _(' with dilated convolution in some layers and without the last pooling and classification fully connected layers.  As a decoder, ') }}
                    <a href="https://arxiv.org/pdf/1706.05587.pdf">DeepLabV3</a>
                    {{ _(' from Google is used. The contribution of DeepLab is the introduction of atrous convolutions, or dilated convolutions, to extract more dense features where information is better preserved given objects of varying scale. And main feature in DeepLab decoder is so called "Atrous Spatial Pyramid Pooling" - a complicated module that consist of one 1x1 conv module, three 3x3 conv modules with dilation rates 12, 24 and 36 and Adaptive Pooling module. Encoder output is fed to all of these modules simultaneously. Then all these modules outputs are concatenated and passed through several convolutional layers. Final 1x1 convolution layer is used to match number of channels to number of classes.') }}
                    </p>
                    <p>{{ _('During the training of this network, also auxiliary FCN classifiers can be used. ') }}
                    {{ _(' In this case, additional outputs from the intermediate layers of the encoder are fed to separate FCN classifiers. The loss function in this case is the weighted sum of the losses of each of these classifiers and the final DeepLab head loss. ') }}
                    </p>
                    <p>
                    {{ _('Typically, the decoder outputs have a dimension less than that of the input image. Bilinear upsampling is used to match the dimension of the output semantic mask with the size of the input image.') }}
                    </p>
                    <p>{{ _('You can read more about DeepLab type models for semantic segmentation ') }}
                    <a href="https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">{{ _('here') }}</a>
                    {{ _('and') }}
                    <a href="https://towardsdatascience.com/deeplabv3-c5c749322ffa">{{ _('here') }}</a>
                    .
                    </p>
                </div>
            </div>
            </div>
            <!-- ResNet+Deeplabv3 ADE20K -->
            <div class="tab-pane fade" id="model3-href" role="tabpanel" aria-labelledby="model3-tab">
            <div class="card">
                <div class="card-header" style="">
                    {{ _('Model is pretrained on ADE20K dataset (150 classes). Experimental!') }}
                </div>
                <div class="card-body">
                    <p class="card-text">
                    {{ _('ResNet101+DeepLab is another example of a network for semantic segmentation of images. It consists, like many others, of two main modules: an encoder and a decoder (sometimes called a backbone and a classifier respectively). The encoder of this model, as you can see from the its caption, is also well-known model for image recognition ') }}
                    <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet101</a>
                    {{ _(' with dilated convolution in some layers and without the last pooling and classification fully connected layers.  As a decoder, ') }}
                    <a href="https://arxiv.org/pdf/1706.05587.pdf">DeepLabV3</a>
                    {{ _(' from Google is used. The contribution of DeepLab is the introduction of atrous convolutions, or dilated convolutions, to extract more dense features where information is better preserved given objects of varying scale. And main feature in DeepLab decoder is so called "Atrous Spatial Pyramid Pooling" - a complicated module that consist of one 1x1 conv module, three 3x3 conv modules with dilation rates 12, 24 and 36 and Adaptive Pooling module. Encoder output is fed to all of these modules simultaneously. Then all these modules outputs are concatenated and passed through several convolutional layers. Final 1x1 convolution layer is used to match number of channels to number of classes.') }}
                    </p>
                    <p>{{ _('During the training of this network, also auxiliary FCN classifiers can be used. ') }}
                    {{ _(' In this case, additional outputs from the intermediate layers of the encoder are fed to separate FCN classifiers. The loss function in this case is the weighted sum of the losses of each of these classifiers and the final DeepLab head loss. ') }}
                    </p>
                    <p>
                    {{ _('Typically, the encoder outputs have a dimension less than that of the input image. Bilinear upsampling is used to match the dimension of the output semantic mask with the size of the input image.') }}
                    </p>
                    <p>
                    {{ _('ADE20 dataset provides images of objects divided into 150 classes. Such a number of classes is a real challenge for the successful training of semantic segmentation models. In addition, one of the problems of this dataset, in my opinion, is the human factor and the markup problem. In the dataset, images of objects belonging to very semantically similar classes are quite common, but are marked with different labels, which can make it difficult to accurately identify them. For example, “building”, “house”, “skyscraper”, “wall” - these are different classes in this dataset, as well as “sea” and “water”. In this regard, you should not expect from the models given here, pre-trained on this dataset, special accuracy of work.') }}
                    </p>
                    <p>{{ _('You can read more about DeepLab type models for semantic segmentation ') }}
                    <a href="https://towardsdatascience.com/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">{{ _('here') }}</a>
                    {{ _('and') }}
                    <a href="https://towardsdatascience.com/deeplabv3-c5c749322ffa">{{ _('here') }}</a>

                    .
                    </p>
                </div>
            </div>
            </div>
            <!-- HRNET ADE20K -->
            <div class="tab-pane fade" id="model4-href" role="tabpanel" aria-labelledby="model4-tab">
            <div class="card">
                <div class="card-header" style="">
                    {{ _('Model is pretrained on ADE20K dataset (150 classes). Experimental!') }}
                </div>
                <div class="card-body">
                    <p class="card-text">
                    <a href="https://arxiv.org/pdf/1904.04514.pdf">HRNet</a>
                    {{ _(' (highresolution network) - is a great example of a complicated network for semantic segmentation (object detection and some others tasks) of images. The core idea behind HRNet is to not only use a single resolution as ResNet, but to keep multiple resolutions, compute with them in parallel and share the information (fusion) across the different resolutions. ') }}
                    {{ _('It is started from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one, forming new stages, and connect the multi-resolution subnetworks in parallel. As a result, the resolutions for the parallel subnetworks of a later stage consists of the resolutions from the previous stage, and an extra lower one ') }}
                    {{ _('This network structure significantly increases the required computing power. In addition, the network shows better results when processing higher resolution images. Given that this introductory site uses GPU-free hosting, the resolution of input images is underestimated, which, among other factors, reduces the accuracy of the network. ') }}
                    </p>
                    <p>
                    {{ _('ADE20 dataset provides images of objects divided into 150 classes. Such a number of classes is a real challenge for the successful training of semantic segmentation models. In addition, one of the problems of this dataset, in my opinion, is the human factor and the markup problem. In the dataset, images of objects belonging to very semantically similar classes are quite common, but are marked with different labels, which can make it difficult to accurately identify them. For example, “building”, “house”, “skyscraper”, “wall” - these are different classes in this dataset, as well as “sea” and “water”. In this regard, you should not expect from the models given here, pre-trained on this dataset, special accuracy of work.') }}
                    </p>
                    <p>{{ _('You can read more about HRNet ') }}
                    <a href="https://github.com/HRNet/HRNet-Semantic-Segmentation">{{ _('here') }}</a>
                    .
                    </p>
                </div>
            </div>
            </div>
        </div>
    </div>
    <!-- /Model change form -->
</div>


{% endblock %}


