{% extends "base.html" %}

{% block app_content %}

<div class="">

    <!-- Modal page info -->
    <div class="modal fade" id="PageModalDescription" tabindex="-1" role="dialog">
    <div class="modal-dialog modal-xl modal-dialog-centered" role="document">
        <div class="modal-content">
          <div class="modal-header">
            <p class="modal-title" style="font-size: calc(12px + 1.1vw); font-weight: bold">{{ _('Object Detection') }}</p>
            <button type="button" class="close" data-dismiss="modal" aria-label="Close">
              <span aria-hidden="true">&times;</span>
            </button>
          </div>
          <div class="modal-body text-justify" style="font-size: 0.8rem; line-height: 1.1; font-family: Arial">
              <p class="">
                  <a href="https://en.wikipedia.org/wiki/Object_detection">{{ _('Object detection') }}</a>
                  {{ _(' is perhaps the most difficult task of computer vision, the essence of which is to detect known objects in the image, determine which known classes these objects belong to, and also determine their location. In general, the result of the work of a neural network that solves the problem of object detection is the bounding boxes around various objects with class name labels. As a logical continuation of the image detection task, the Instance segmentation task is considered - pixel-wise selection of objects in the image; moreover, unlike semantic segmentation, different objects of the same class are distinguished.') }}
                  {{ _('Of course, all modern methods for solving problems of object detection (and instance segmentation) use the artificial neural networks, convolutional neural networks in particular. ') }}
                  {{ _('Various sources and articles describe in some detail the architectures of various artificial neural networks models for detecting objects, starting with one of the very first ') }}
                  <a href="https://arxiv.org/pdf/1311.2524.pdf">R-CNN</a>
                  {{ _(' which marked the beginning of a series of continuing improvements and enhancements, which resulted in the appearance of such models, for example, as ') }}
                  <a href="https://arxiv.org/pdf/1504.08083.pdf">Fast R-CNN</a> {{ _(', ') }}
                  <a href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-CNN</a> {{ _(', ') }}
                  <a href="https://arxiv.org/pdf/1506.02640.pdf">YOLO</a> {{ _(', ') }}
                  <a href="https://arxiv.org/pdf/1512.02325.pdf">SSD</a>
                  {{ _(' and others.') }}
              </p>
              <p> {{ _('The basic principle of operation of region-based object detection models is to find out the so-called Regions Of Interest (RoI), to recognize an object and a background in each of the regions, to make a linear regression of the coordinates of these regions. The differences between the models are precisely in the ways of implementing the above. In the article ') }}
                  <a href="https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/">{{ _('A Step-by-Step Introduction to the Basic Object Detection Algorithms') }}</a>
                  {{ _(' you can read more about it.') }}
              </p>
              <p> {{ _("Instance segmentation is a kind of a combination of the tasks of object detection and semantic segmentation. After, using the models of object detection, the bounding boxes and labels of the classes of objects in them are defined, a semantic analysis of each area of the image in these bounding boxes is performed. The most famous model of an artificial neural network that solves this problem is ") }}
                  <a href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN</a>.
              </p>
              <p> {{ _('The most commonly used dataset used to train image detection and instance segmentation models is ') }}
                  <a href="http://cocodataset.org/#home">{{ _('COCO 2017') }}</a>
              </p>
              <p> {{ _('On this page there is an area for previewing the image, in this area predicted bounding boxes and labels will be displayed, according to the used model. You can upload your image with "Browse" button or paste the image from the clipboard or make a shot with a camera. You can also switch the models of neural networks using the corresponding buttons in the bottom. The prediction results will change automatically.') }}
              </p>
              <p style="color: red; font-weight: bold"> {{ _('Please note that this demo site is not hosted on a high-performance computer, so it will take some time to operate, be patient.') }}
              </p>
          </div>
        </div>
    </div>
    </div>
    <!-- /Modal page info -->

    <!-- Brief page description on top-->
    <div class="card" style="padding: 0; margin: 5px 0 0 0; ">
        <div class="card-body" style="padding: 5px 5px 0 5px; margin: 5px 5px -5px 5px;">
<!--        <button class="btn btn-primary fa fa-question-circle fa-siz fa-2x" id="page-help" style="float: right; padding: 0; margin: 0px 0px 3px 10px;"> </button>  -->
        <h6 class="text-justify" id="description-text" style="font-family: Yanone Kaffeesatz; font-size: calc(10px + 0.5vw) ;cursor: pointer;  line-height: 0.95;">
            {{ _('Select a file or paste an image from the clipboard or make a foto with a camera and an appropriate neural network model will try to detect objects in this image. You can switch between models using the corresponding buttons with models\' names below. ') }}
            <span style="color: red">{{ _('You can click this message for more details.') }} </span>
        </h6>
        </div>
    </div>
    <!-- /Brief page description on top-->

    <!-- Input form -->
    <form style="display: block; position: relative;margin: 1rem 0" id="upload-file" method="post" enctype="multipart/form-data">
      <div class="block-input-modal" style=""></div> <!-- Block input form -->
      <div class="d-flex" style="justify-content: center; justify-items: center; ">
        <label class="btn btn-outline-primary text-nowrap" style="" for="inputfile" id="inputlabel1"> {{ _('Browse file') }} </label>
        <input id="inputfile" type="file" style="display: none;" name="file" class="form-control-file"  accept="image/*">
        <input type="url" id="inputurl" class="form-control" style="padding: 0; height: auto" name="img-url" placeholder="{{ _('or paste image here') }}" />
        <label class="btn btn-outline-primary" style="" for="camera-input" id="inputlabel2"> <span data-feather="camera"></span></label>
        <input id="camera-input" type="file" style="display: none;" accept="image/*" capture="camera" />
      </div>
    </form>
    <!-- /Input form -->

    <!-- Preview and predict form -->
    <div class="d-flex flex-row justify-content-between preview-predict-form" style="">
        <div class="block-input-modal" style="position: absolute; width: 100%; height: 100%">
         <div style="position: absolute; top: 50%; left: 50%; transform: translateX(-50%) translateY(-50%); font-size: 3vw; font-weight: 600 ;">
             <span class="spinner-grow spinner-grow-lg" style="vertical-align: middle" role="status" aria-hidden="true"></span>
             <span class="wait-text-blinking">{{ _('Please wait') }}</span>
         </div>
        </div>
        <!-- Image preview and predicted image -->
        <div class="" style="height: 100%; width: 70%; margin-right: 0">
            <div class="image-in-preview"> <!-- IMAGE -->
                <img class="rounded" src="" id="imgpreview" alt="" style="height: 100%; width: auto; z-index: 0;">
            </div>
            <div class="image-in-preview"> <!-- SEGMENTED IMAGE -->
                <img class="rounded" src="" id="img_seg" alt="" style="height: 100%; width: auto; z-index: 1; opacity: 0.0">
            </div>
            <div class="image-in-preview"> <!-- CANVAS -->
                <canvas id="detection_canvas" style="height: 100%; width: auto; z-index: 2; opacity: 0.0;"></canvas>
            </div>
        </div>
    </div>
    <!-- /Preview and predict form -->

    <!-- Model change form -->
    <div class="d-block" style="position: relative;">
        <div class="f-flex block-input-modal" style=" "></div> <!-- Block models change -->
        <ul class="nav nav-pills" id="models-tab" role="tablist">
            <li class="nav-item">
                <a class="nav-link active" id="model1-tab" data-target="#model1-href" data-toggle="pill" href="./model?model=maskrcnn_resnet50_fpn_coco"  role="tab" aria-controls="model1-href" aria-selected="true">MaskRCNN_Coco</a>
            </li>
        </ul>
        <div class="tab-content models-tab" id="models-tabContent" style="">
            <!-- MaskRCNN_ResNet50_FPN coco -->
            <div class="tab-pane fade show active" id="model1-href" role="tabpanel" aria-labelledby="model1-tab">
            <div class="card">
                <div class="card-header" style="">
                    {{ _('This model is pretrained on COCO 2017 dataset (91 classes) and performs object detection and instance segmentation tasks.') }}
                </div>
                <div class="card-body">
                    <p class="card-text">
                        {{ _('MaskRCNN_ResNet50_FPN - a PyTorch torchvision zoo model for detecting objects and instance segmentation built on the basis of another model - Faster RCNN, so first we will consider that model. The whole Faster RCNN model consisting of two main parts: Backbone and FasterRCNN itself. The input image (after necessary transformations) is fed to the backbone input, which produces a set of spatial features, which, in turn, are fed to the FasterRCNN input, where, in fact, the detection of objects is carried out.') }}
                    </p>
                    <p class="card-text">
                        {{ _('As the backbone (submodel for extracting feature maps from the input image), the well-known ') }}
                        <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet50</a>
                        {{ _('  with an "add-in" in the form of ') }}
                        <a href="https://arxiv.org/pdf/1612.03144.pdf">Feature Pyramid Network</a>
                        {{ _(' (FPN) is used here. You also can read more about FPN ') }}
                        <a href="https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c">{{ _('here') }}</a>
                        {{ _('. In short, we take feature maps from four "global" ResNet layers (Layer 1-4), convert to 256 (default) channels, using a convolutional layer with a 1x1 kernel; add the upper layer interpolated to the size of the current (starting from the penultimate layer) then we apply one more convolution with a 3x3 kernel and send the result to the output. That is. For the four layers of ResNet50, we have 4 outputs:') }}
                    </p>
                    <ul>
                        <li> {{ 'Layer4 -> Conv1x1 -> Conv3x3 -> Out[\'3\']' }} </li>
                        <li> {{ 'Layer3 -> Conv1x1+(Interpolate(Layer4 -> Conv1x1)) -> Conv3x3 -> Out[\'2\']' }} </li>
                        <li> {{ 'Layer2 -> Conv1x1+(Interpolate(Layer3 -> Conv1x1)) -> Conv3x3 -> Out[\'1]\'' }} </li>
                        <li> {{ 'Layer1 -> Conv1x1+(Interpolate(Layer2 -> Conv1x1)) -> Conv3x3 -> Out[\'0\']' }} </li>
                    </ul>
                    <p class="card-text">
                        {{ _('And we add one more output for Max Pooling:') }}
                    </p>
                    <ul>
                        <li> {{ 'Out[0] -> MaxPool2d -> Out[\'pool\']' }} </li>
                    </ul>
                    <p class="card-text">
                        {{ _('As we "go up", the spatial resolution decreases. For example, with an input image resolution of 640x480, the backbone used will produce 5 feature maps with 256 (default) channels each with resolutions of [160,120], [80, 60], [40, 30], [20, 15] and [10, 8] from Out[\'0\'] to Out[\'pool\'].') }}
                        {{ _('In total, a list (actually a dictionary that is converted later to a list) of spatial features of five different sizes is fed to the input of FasterRCNN. This approach improves the accuracy of detection of small objects and increasing the overall accuracy of the model.') }}
                    </p>
                    <p class="card-text">
                        {{ _('The second main module of the model is ') }}
                        <a href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-CNN</a>
                        {{ _(', which has a more complex structure with several modules and hyperparameters, which are described in more detail later in the text.') }}
                    </p>
                    <p class="card-text">
                        <a href="https://medium.com/egen/region-proposal-network-rpn-backbone-of-faster-r-cnn-4a744a38d7f9">Region Proposal Network (RPN):</a>
                        {{ _('RPN is a sub-module of FasterRCNN used to determine the so-called Regions of Interest (RoI), which are rectangular areas of the image in which the desired objects are most likely to be located.') }}
                        {{ _('First, we need to generate the so-called anchors, which are the "support" rectangles of different scales and aspect ratios, relative to which the bounding boxes of the found objects will then be determined.') }}
                        {{ _('Anchors are generated for each feature map separately, their sizes and aspect ratios are set in advance during initialization. In our case, sizes = ((32,), (64,), (128,), (256,), (512,)) - respectively for 5 feature maps; aspect ratios = (0.5, 1.0, 2.0) for each of the sizes.') }}
                        {{ _('Thus, for each feature map, for each "node" of the spatial grid, 3 anchors are generated. The centers of these anchors are evenly distributed over the entire spatial size of the input image, starting from the upper left corner, in increments equal to the ratio of the size of the input image to the size of the feature map for which the anchors are generated.') }}
                        {{ _('In other words, the anchors centers are located, as it were, in the nodes of the spatial grid formed by the corresponding feature map. Since the sizes of feature maps are different, the number of anchors per feature map is different also.') }}
                        {{ _('For example, for the above size of the input image (640x480) and the backbone used, for the Out [0] feature map with size 160x120, the number of anchors will be 160x120x3 = 57600. For the described procedure for generating anchors in this implementation, the AnchorGenerator module is used.') }}
                        {{ _('<br>The next module included in RPN is RPNHead, which is a small convolutional subnet of one 3x3 layer and two separate 1x1 layers. Feature maps from the backbone are fed to the RPNHead input, and at the output we get the so-called "objectness score" - a certain measure of whether an object in the corresponding bounding box belongs to a set of known classes or to a background (background); and "bounding box regression" ("bounding box deltas") - the "encoded" (according to equation (2) of original ') }}
                        <a href="https://arxiv.org/pdf/1506.01497.pdf">Faster R-CNN</a>
                        {{ _(' paper) offset of the proposal bounding boxes (or RoI) from the corresponding anchors. <br>Having bounding box deltas from RPNHead and anchor coordinates from AnchorGenerator, we calculate (according to Eqn.2 that mentioned before) the coordinates of the proposed bounding boxes, or RoI). Then we take Top-N according to objectness score (hyperparameter pre_nms_top_n, - by default for train mode 2000, for test - 1000) proposals and objectness for each feature map separately, crop them to fit the image, delete if the size after cropping is too small. The remaining proposals and objectness are filtered out again using ') }}
                        <a href="https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c">Non-maximum Suppression (NMS)</a>
                        {{ _(' using proposals, objectness and the nms_thresh hyperparameter (default 0.7). From the resulting list, sorted in descending order of objectness scores by all feature maps, we save the first post_nms_top_n (by default for train mode 2000, for test mode 1000) values. In total, at the output of RPN we have Tensor[post_nms_top_n, 4] with the coordinates of the proposal bounding boxes - Regions of Interest (RoI).') }}
                        {{ _('<br>The original FasterRCNN uses RoIPool to extract features from feature maps in each RoI and bring them all to the same size (7x7 by default). The model we use here uses RoIAlign described in the ') }}
                        <a href="https://arxiv.org/pdf/1703.06870.pdf">Mask-RCNN</a>
                        {{ _(' paper. You can read more about both methods ') }}
                        <a href="https://medium.com/@andrewjong/how-to-use-roi-pool-and-roi-align-in-your-neural-networks-pytorch-1-0-b43e3d22d073">{{ _('here') }}</a>
                        {{ ', ' }}
                        <a href="https://tjmachinelearning.com/lectures/1718/instance/instance.pdf">{{ _('here') }}</a>
                        {{ _('and ') }}
                        <a href="https://chao-ji.github.io/jekyll/update/2018/07/20/ROIAlign.html">{{ _('here') }}</a>
                        {{ '.' }}
                        {{ _('<br>The model uses MultiScaleRoIAlign - a torchvision submodule that performs the roi_align operation, accepts RoI, generated by RPN and feature maps from the backbone as inputs. The output of this submodule for one input image of 640x480 size and the value post_nms_top_n = 1000 will be the shape Tensor[1000, 256, 7, 7]. ') }}
                        {{ _('The features obtained using RoIAlign for each RoI are fed to a two-layer MultiLayerPerceptron MLP (BoxHead), converting 1000 RoI features (256 x 7 x 7) into a “flat” representation of dimension 1024. This representation then goes to the FastRCNNPredictor module, which is two single-layer MLPs, one for an labels (object class), another for the box coordinates deltas. The output of this submodule, respectively, is \"class_logits\" - scores of classes for each bounding box and \"box_regression\" - offsets of these bounding boxes (relative to RoI generated by RPN) for each object class. That is, if we consider one bounding box, then in class_logits[0] there will be scores for each of the known classes (in our case, 91), and in box_regression there will be 91 values ​​of the offsets of the x, y coordinates, width and height of bounding box with respect to RoI from RPN.') }}
                        {{ _('Again with the Eqn.2 from Faster-RCNN paper we \"decode\" box_regression into real boxes coordinates, remove elements with a low scores value, crop them to the actual size of the image, delete too small boxes, filter using NMS (Non-maximum Suppression). We compose the result in a list of dictionaries for each input image. The dictionary contains the coordinates of the bounding boxes for each object, the class of this object, and scores (detection confidence) of the class of the object.') }}
                        {{ _('<br>The pytorch implementation described above is quite complicated and confusing, but you can always look at it in detail in the ') }}
                        <a href="https://github.com/pytorch/vision/blob/4897402a1730b96043edbc2f3c5564075572c26c/torchvision/models/detection/faster_rcnn.py">{{ _('source') }}</a>
                        {{ _('.') }}
                    </p>
                    <p class="card-text">
                        {{ _('Now we consider what exactly Mask RCNN adds to FasterRCNN to make it possible to perform instance segmentation task. ') }}
                        {{ _('From FasterRCNN block we have bounding boxes for each known object instance in the image and labels of objects in these bounding boxes. Also we have feature maps from the backbone. MaskRCNN (as well as FasterRCNN from above) has RoIPool submodule (MultiScaleRoIAlign class), Head-module (MaskRCNNHead) and Predictor module (MaskRCNNPredictor). MaskRoIPool is almost the same as FasterRCNN has (described upper) with only difference in output feature size. This submodule takes features maps from a backbone and bounding boxes from FasterRCNN and outputs "mask_features" of the same size for each bounding box. ') }}
                        {{ _('MaskRCNNHead (which is a four-layers fully conolutional network with 3x3 kernel and ReLU activation) takes this pooled features as input. MaskRCNNPredictor (one transposed convolution layer with 2x2 kernel and stride 2 and one convolution layer with 1x1 kernel) takes MaskRCNNHead outputs and "convert" them to class probabilities for each known object class. After that we have a tensor of shape like this: [NumberOfBoxes, 91, 28, 28], where 91 - number of known classes, 28 - spartial size. Then we select mask probabilties according to the labels (from FasterRCNN) and get mask probabilities of correct class. Finally we resize masks to original image sizes.') }}
                        {{ _('<br>You can look at it all with your own eyes in the ') }}
                        <a href="https://github.com/pytorch/vision/blob/master/torchvision/models/detection/mask_rcnn.py">{{ _('source') }}</a>
                        {{ '.' }}
                    </p>


                </div>
            </div>
            </div>
        </div>
    </div>
    <!-- /Model change form -->
</div>


{% endblock %}


